{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## MedMCQA ‚Äî LoRA fine-tune of **Meta-Llama-3-8B-Instruct** (Unsloth + QLoRA)\n\n**Goal.** Train a tutor to pick the correct A/B/C/D option on MedMCQA and (optionally) give a short explanation.\n\n**Dataset.** `openlifescienceai/medmcqa` (train/validation splits from the HF dataset).\n\n**Base model.** `meta-llama/Meta-Llama-3-8B-Instruct`  \n*(A separate notebook covers Qwen2.5-7B-Instruct.)*\n\n**Method.** Unsloth + QLoRA (bnb NF4) + LoRA adapters\n\n**LoRA hyperparams actually used here.** `r=32`, `alpha=64`, `dropout=0.0`  \n**Max sequence length.** `768` (reduced from 1024 to fit T4 16 GB during fine-tuning)  \n**Hardware.** Kaggle T4 (16 GB)\n\n> ‚ö†Ô∏è Educational use only. Not medical advice.","metadata":{}},{"cell_type":"code","source":"%%capture\n\n!pip install unsloth # install unsloth\n!pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git # Also get the latest version Unsloth!","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T18:55:41.330296Z","iopub.execute_input":"2025-08-21T18:55:41.330584Z","iopub.status.idle":"2025-08-21T19:01:14.585385Z","shell.execute_reply.started":"2025-08-21T18:55:41.330561Z","shell.execute_reply":"2025-08-21T19:01:14.584217Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"## Import all relevant packages throughout this walkthrough","metadata":{}},{"cell_type":"code","source":"import os, gc\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\nos.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T19:01:14.586981Z","iopub.execute_input":"2025-08-21T19:01:14.587246Z","iopub.status.idle":"2025-08-21T19:01:14.592463Z","shell.execute_reply.started":"2025-08-21T19:01:14.587217Z","shell.execute_reply":"2025-08-21T19:01:14.591885Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Modules for fine-tuning\nfrom unsloth import FastLanguageModel\nimport torch # Import PyTorch\nfrom trl import SFTTrainer # Trainer for supervised fine-tuning (SFT)\nfrom unsloth import is_bfloat16_supported # Checks if the hardware supports bfloat16 precision\n# Hugging Face modules\nfrom huggingface_hub import login # Lets you login to API\nfrom transformers import TrainingArguments, EarlyStoppingCallback # Defines training hyperparameters\nfrom datasets import load_dataset # Lets you load fine-tuning datasets\n# Import weights and biases\nimport wandb\n# Import kaggle secrets\nfrom kaggle_secrets import UserSecretsClient\nfrom functools import partial","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T19:01:14.593457Z","iopub.execute_input":"2025-08-21T19:01:14.593734Z","iopub.status.idle":"2025-08-21T19:02:00.037543Z","shell.execute_reply.started":"2025-08-21T19:01:14.593711Z","shell.execute_reply":"2025-08-21T19:02:00.036802Z"}},"outputs":[{"name":"stdout","text":"ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-08-21 19:01:24.301201: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755802884.683296      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755802884.796989      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ü¶• Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import random\n\n# reproducibility\ntorch.manual_seed(3407)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(3407)\nrandom.seed(3407)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T19:02:00.039715Z","iopub.execute_input":"2025-08-21T19:02:00.039979Z","iopub.status.idle":"2025-08-21T19:02:00.046331Z","shell.execute_reply.started":"2025-08-21T19:02:00.039963Z","shell.execute_reply":"2025-08-21T19:02:00.045843Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Initialize Hugging Face & WnB tokens\nuser_secrets = UserSecretsClient() # from kaggle_secrets import UserSecretsClient\nhugging_face_token = user_secrets.get_secret(\"Hugging_Face_Token\")\nwnb_token = user_secrets.get_secret(\"wnb\")\n\n# Login to Hugging Face\nlogin(hugging_face_token) # from huggingface_hub import login\n\n# Login to WnB\nwandb.login(key=wnb_token) # import wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T19:02:14.719816Z","iopub.execute_input":"2025-08-21T19:02:14.720483Z","iopub.status.idle":"2025-08-21T19:02:20.997555Z","shell.execute_reply.started":"2025-08-21T19:02:14.720455Z","shell.execute_reply":"2025-08-21T19:02:20.996943Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mavpk\u001b[0m (\u001b[33mavpk-university-of-waterloo\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"## Load Llama-3-8B-Instruct and Tokenizer\n\nWe load the base model with Unsloth's `FastLanguageModel.from_pretrained()` and enable 4-bit (NF4) quantization to save VRAM.\n\n**Key knobs:**\n- `max_seq_length = 768`\n- `load_in_4bit = True` (bitsandbytes NF4 via Unsloth)\n- `gpu_memory_utilization = 0.45`\n\n**Intuition behind 4-bit quantization**\n\nImagine compressing a **high-resolution image** to a smaller size‚Äî**it takes up less space but still looks good enough**. Similarly, **4-bit quantization reduces the precision of model weights**, making the model **smaller and faster while keeping most of its accuracy**. Instead of storing precise **32-bit or 16-bit numbers**, we compress them into **4-bit values**. This allows **large language models to run efficiently on consumer GPUs** without needing massive amounts of memory. ","metadata":{}},{"cell_type":"code","source":"# ==== Base model for this notebook (Llama-3-8B-Instruct) ====\nbase_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n\nmax_seq_length = 768\ndtype = None\nload_in_4bit = True  # QLoRA / NF4 via bitsandbytes under Unsloth\n\ndef load_base(model_name):\n    model, tokenizer = FastLanguageModel.from_pretrained(\n        model_name=model_name,\n        max_seq_length=max_seq_length,          \n        dtype=dtype,\n        load_in_4bit=load_in_4bit,\n        token=hugging_face_token,\n        device_map={\"\": torch.cuda.current_device()} if torch.cuda.is_available() else {\"\": \"cpu\"},\n        gpu_memory_utilization=0.45,             \n        low_cpu_mem_usage=True,                 \n    )\n    return model, tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T19:02:21.776151Z","iopub.execute_input":"2025-08-21T19:02:21.777082Z","iopub.status.idle":"2025-08-21T19:02:21.781644Z","shell.execute_reply.started":"2025-08-21T19:02:21.777055Z","shell.execute_reply":"2025-08-21T19:02:21.781025Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## Quick baseline check before fine-tuning\n\nWe sanity-check prompt formatting and then compute a small decode-free baseline with eval_mcq_logits. This is just to verify prompt formatting and tokenization before training.\n","metadata":{}},{"cell_type":"markdown","source":"### Running a quick baseline\n\nWe compute a small validation baseline with a **decode-free, answer-only scorer** (`eval_mcq_logits`), which scores the first generated token‚Äôs log-prob among {A,B,C,D}. This matches our training format and is much faster than long decoding.\n\nLater, we also sample a **small batch of short explanations** (a few dozen tokens) purely for qualitative inspection.\n","metadata":{}},{"cell_type":"markdown","source":"### Step 1 ‚Äî Load and lightly format MedMCQA\n\nWe use `openlifescienceai/medmcqa` directly (HF Datasets).  \nWe format each example into a short chat conversation and train with **answer-only supervision** (letter A/B/C/D).\n","metadata":{}},{"cell_type":"code","source":"# MedMCQA splits (train/val/test). \n\nraw = load_dataset(\"openlifescienceai/medmcqa\")\ntrain_ds = raw[\"train\"]\nval_ds_orig   = raw[\"validation\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T19:02:23.718452Z","iopub.execute_input":"2025-08-21T19:02:23.718776Z","iopub.status.idle":"2025-08-21T19:02:29.597982Z","shell.execute_reply.started":"2025-08-21T19:02:23.718752Z","shell.execute_reply":"2025-08-21T19:02:29.597272Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"607fbef4317940a0a8e14c88bd466782"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/85.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e318f20b0d2448eb9fb7867a4e3006e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/test-00000-of-00001.parquet:   0%|          | 0.00/936k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bab078e0a85149058c4753f5025575af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/validation-00000-of-00001.parquet:   0%|          | 0.00/1.48M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f71b3fb542d044f7ae9ee8148fe94c51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/182822 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc49915fdfc94833b57457762efe6ded"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/6150 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47394ed484124a229e6004a0d70b060a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/4183 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98264292e2484355a1bcb904d9c79ed6"}},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"def to_text_examples(batch, tokenizer, eos_token=None):\n    texts = []\n    for q, a, b, c, d, gold in zip(batch[\"question\"], batch[\"opa\"], batch[\"opb\"], batch[\"opc\"], batch[\"opd\"], batch[\"cop\"]):\n        gold_letter = gold_to_letter(gold) #or \"A\"\n        user = (\n            \"You are a medical expert. Answer this MCQ with a single letter.\\n\\n\"\n            f\"Question:\\n{q}\\n\\nOptions:\\nA. {a}\\nB. {b}\\nC. {c}\\nD. {d}\\n\\n\"\n            \"Respond in the format:\\nAnswer: <A/B/C/D>\"\n        )\n        assistant = f\"Answer: {gold_letter}\"\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a medical expert.\"},\n            {\"role\": \"user\", \"content\": user},\n            {\"role\": \"assistant\", \"content\": assistant},\n        ]\n        txt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n        texts.append(txt)\n    return {\"text\": texts}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T19:02:29.599199Z","iopub.execute_input":"2025-08-21T19:02:29.599452Z","iopub.status.idle":"2025-08-21T19:02:29.605155Z","shell.execute_reply.started":"2025-08-21T19:02:29.599436Z","shell.execute_reply":"2025-08-21T19:02:29.604381Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"### Step 2 ‚Äî Setting up the model using LoRA\n\n**An intuitive explanation of LoRA** \n\nLarge language models (LLMs) have **millions or even billions of weights** that determine how they process and generate text. When fine-tuning a model, we usually update all these weights, which **requires massive computational resources and memory**.\n\nLoRA (**Low-Rank Adaptation**) allows to fine-tune efficiently by:\n\n- Instead of modifying all weights, **LoRA adds small, trainable adapters** to specific layers.  \n- These adapters **capture task-specific knowledge** while leaving the original model unchanged.  \n- This reduces the number of trainable parameters **by more than 90%**, making fine-tuning **faster and more memory-efficient**.  \n\nThink of an LLM as a **complex factory**. Instead of rebuilding the entire factory to produce a new product, LoRA **adds small, specialized tools** to existing machines. This allows the factory to adapt quickly **without disrupting its core structure**.\n\nBelow, we will use the `get_peft_model()` function which stands for Parameter-Efficient Fine-Tuning ‚Äî this function wraps the base model (`model`) with LoRA modifications, ensuring that only specific parameters are trained.\n\n**This notebook uses:** `r=32`, `alpha=64`, `dropout=0.0` and targets attention/MLP projections (`q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj`).\n","metadata":{}},{"cell_type":"code","source":"def add_lora(model):\n    return FastLanguageModel.get_peft_model(\n        model,\n        r=32,\n        target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n        lora_alpha=64,                \n        lora_dropout=0.0,         \n        bias=\"none\",\n        use_gradient_checkpointing=\"unsloth\",\n        random_state=3407,\n        use_rslora=False,\n        loftq_config=None,\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T19:02:29.605880Z","iopub.execute_input":"2025-08-21T19:02:29.606120Z","iopub.status.idle":"2025-08-21T19:02:29.626355Z","shell.execute_reply.started":"2025-08-21T19:02:29.606099Z","shell.execute_reply":"2025-08-21T19:02:29.625837Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# where make_trainer is defined\ndef make_trainer(model_lora, tokenizer, dataset, outdir, eval_dataset=None):\n    return SFTTrainer(\n        model=model_lora,\n        tokenizer=tokenizer,\n        train_dataset=dataset,\n        eval_dataset=eval_dataset,           \n        dataset_text_field=\"text\",\n        max_seq_length=768,\n        dataset_num_proc=2,\n        packing=False,\n        args=TrainingArguments(\n            per_device_train_batch_size=1,\n            per_device_eval_batch_size=2,\n            gradient_accumulation_steps=8,   \n            num_train_epochs=2,               # let early stopping pick best\n            learning_rate=1e-4,               # LoRA likes a bit higher LR\n            warmup_ratio=0.05,\n            dataloader_num_workers=2,\n            lr_scheduler_type=\"cosine\",\n            weight_decay=0.0,\n            gradient_checkpointing=True,      \n            max_grad_norm=1.0,                # clip for stability\n            fp16=not is_bfloat16_supported(),\n            bf16=is_bfloat16_supported(),\n            logging_steps=50,\n            optim=\"adamw_8bit\",               # good with 4-bit base\n            group_by_length=True,\n            output_dir=outdir,\n            report_to=\"wandb\",\n            seed=3407,\n            data_seed=3407,\n        ),\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T19:02:29.627941Z","iopub.execute_input":"2025-08-21T19:02:29.628323Z","iopub.status.idle":"2025-08-21T19:02:29.644359Z","shell.execute_reply.started":"2025-08-21T19:02:29.628307Z","shell.execute_reply":"2025-08-21T19:02:29.643807Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import re\nimport numpy as np\nfrom torch.utils.data import Subset\n\nnp.random.seed(3407)\n\n# Answer-only prompt (no explanation at eval time)\nANSWER_ONLY_PROMPT = \"\"\"You are a helpful medical AI.\nQuestion: {question}\n\nOptions:\nA. {opa}\nB. {opb}\nC. {opc}\nD. {opd}\n\nRespond with exactly one letter (A, B, C, or D).\nAnswer: \"\"\"\n\ndef chat_wrap(tokenizer, user_text):\n    # Use proper assistant turn for chatty instruct models (e.g., Llama 3, Qwen)\n    if hasattr(tokenizer, \"apply_chat_template\"):\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a medical expert.\"},\n            {\"role\": \"user\",   \"content\": user_text},\n        ]\n        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    return user_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T19:02:29.645025Z","iopub.execute_input":"2025-08-21T19:02:29.645241Z","iopub.status.idle":"2025-08-21T19:02:29.667218Z","shell.execute_reply.started":"2025-08-21T19:02:29.645226Z","shell.execute_reply":"2025-08-21T19:02:29.666699Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# ===== Quick explanation spot-check (small sample, batched) =====\nEXPLAIN_PROMPT = \"\"\"You are a medical expert. Answer the MCQ and briefly justify in 3‚Äì6 sentences.\n\nQuestion:\n{question}\n\nOptions:\nA. {opa}\nB. {opb}\nC. {opc}\nD. {opd}\n\nRespond in the format:\nAnswer: <A/B/C/D>\nExplanation: <3‚Äì6 sentences>\n\nAnswer: \"\"\"\n\n\nANS_RE = re.compile(r\"Answer\\s*:\\s*([ABCD])\", re.I)\nEXPL_RE = re.compile(r\"Explanation\\s*:\\s*(.*)\", re.I | re.S)\n\ndef format_explain(row, tok):\n    user = EXPLAIN_PROMPT.format(\n        question=row[\"question\"],\n        opa=row[\"opa\"], opb=row[\"opb\"], opc=row[\"opc\"], opd=row[\"opd\"],\n    )\n    return chat_wrap(tok, user)\n\n@torch.no_grad()\ndef sample_explanations(model, tokenizer, dataset, k=8, batch_size=2, max_len=768, new_tokens=160):\n    model.eval()\n    FastLanguageModel.for_inference(model)\n\n    idxs = list(range(min(k, len(dataset))))\n    ds = Subset(dataset, idxs)\n\n    prompts, golds = [], []\n    for i in range(len(ds)):# k items\n        ex = ds[i]\n        prompts.append(format_explain(ex, tokenizer))\n        golds.append(gold_to_letter(ex[\"cop\"]))\n\n    pad_id = tokenizer.pad_token_id or tokenizer.eos_token_id\n    eos_id = tokenizer.eos_token_id\n\n    rows = []\n    for s in range(0, len(prompts), batch_size):\n        batch_prompts = prompts[s:s+batch_size]\n        batch = tokenizer(\n            batch_prompts,\n            return_tensors=\"pt\",\n            padding=\"longest\",          # explicit; respects tokenizer.padding_side=\"left\"\n            truncation=True,\n            max_length=max_len,\n        ).to(model.device)\n\n        out = model.generate(\n            **batch,\n            max_new_tokens=new_tokens,\n            do_sample=False, temperature=0.0, num_beams=1,\n            pad_token_id=pad_id, eos_token_id=eos_id, use_cache=True\n        )\n        \n        gen_only = out[:, batch[\"input_ids\"].shape[1]:]\n        decoded = tokenizer.batch_decode(gen_only, skip_special_tokens=True)\n        for j, txt in enumerate(decoded):\n            m_ans = ANS_RE.search(txt); pred = m_ans.group(1).upper() if m_ans else None\n            m_exp = EXPL_RE.search(txt); expl = m_exp.group(1).strip() if m_exp else \"(no explanation parsed)\"\n            idx = idxs[s + j]; gold = golds[s + j]\n            rows.append((idx, gold, pred, pred == gold, expl, txt))\n\n    # log to W&B if active\n    try:\n        table = wandb.Table(columns=[\"idx\",\"gold\",\"pred\",\"correct\",\"explanation\",\"raw\"])\n        for r in rows: table.add_data(*r)\n        wandb.log({\"sample_explanations\": table})\n    except Exception:\n        pass\n\n    # Console preview (first 3)\n    for idx, gold, pred, correct, expl, _ in rows[:3]:\n        print(f\"[{idx}] gold={gold} pred={pred} correct={correct}\\n  explanation: {expl[:200]}...\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T19:02:29.668074Z","iopub.execute_input":"2025-08-21T19:02:29.668314Z","iopub.status.idle":"2025-08-21T19:02:29.695198Z","shell.execute_reply.started":"2025-08-21T19:02:29.668289Z","shell.execute_reply":"2025-08-21T19:02:29.694686Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"from collections import Counter\nraw_train_cop = Counter(map(int, raw[\"train\"][\"cop\"]))\nraw_val_cop   = Counter(map(int, raw[\"validation\"][\"cop\"]))\nprint(\"[raw cop] train:\", raw_train_cop)\nprint(\"[raw cop] val_orig:\", raw_val_cop)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T19:02:29.695805Z","iopub.execute_input":"2025-08-21T19:02:29.696025Z","iopub.status.idle":"2025-08-21T19:02:29.814439Z","shell.execute_reply.started":"2025-08-21T19:02:29.696007Z","shell.execute_reply":"2025-08-21T19:02:29.813820Z"}},"outputs":[{"name":"stdout","text":"[raw cop] train: Counter({0: 53591, 1: 47826, 2: 42442, 3: 38963})\n[raw cop] val_orig: Counter({0: 1348, 1: 1085, 2: 925, 3: 825})\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"def gold_to_letter(cop):\n    s = str(cop).strip()\n    if s and s[0].upper() in \"ABCD\":\n        return s[0].upper()\n    try:\n        n = int(s)\n    except ValueError:\n        return None\n    return \"ABCD\"[n] if 0 <= n <= 3 else None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T19:02:29.815237Z","iopub.execute_input":"2025-08-21T19:02:29.815488Z","iopub.status.idle":"2025-08-21T19:02:29.828110Z","shell.execute_reply.started":"2025-08-21T19:02:29.815472Z","shell.execute_reply":"2025-08-21T19:02:29.827561Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from transformers.trainer_callback import TrainerCallback\nimport os\n\nclass MCQAccuracyCallback(TrainerCallback):\n    def __init__(self, tokenizer, val_dataset, every=100, patience=8,\n                 max_items=600, log_key=\"val_accuracy\", save_dir=None):\n        self.tokenizer = tokenizer\n        self.val_dataset = val_dataset\n        self.every = every\n        self.patience = patience\n        self.max_items = max_items\n        self.log_key = log_key\n        self.wait = 0\n        self.best_acc = -1.0\n        self.save_dir = save_dir  \n\n    def on_step_end(self, args, state, control, model=None, **kwargs):\n        if state.global_step == 0 or (state.global_step % self.every) != 0:\n            return control\n\n        model.eval()\n        acc = eval_mcq_logits(model, self.tokenizer, self.val_dataset, max_items=self.max_items, batch_size=2, max_len=768)\n        print(f\"[step {state.global_step}] accuracy {acc:.4f}\")\n\n        # optional W&B log\n        try:\n            import wandb\n            wandb.log({self.log_key: acc}, step=int(state.global_step))\n        except Exception:\n            pass\n\n        if self.best_acc < 0 or acc > self.best_acc + 1e-6:\n            self.best_acc = acc\n            self.wait = 0\n            base = self.save_dir or args.output_dir\n            best_dir = os.path.join(base, \"best\")\n            os.makedirs(best_dir, exist_ok=True)\n            print(f\"New best acc={acc:.4f}, saving to {best_dir}\")\n            model.save_pretrained(best_dir)\n            self.tokenizer.save_pretrained(best_dir)\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                print(f\"Early stopping at step {state.global_step}, best={self.best_acc:.4f}\")\n                control.should_training_stop = True\n\n        model.train()\n        FastLanguageModel.for_training(model)\n        \n        return control","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T19:02:30.262978Z","iopub.execute_input":"2025-08-21T19:02:30.263722Z","iopub.status.idle":"2025-08-21T19:02:30.271118Z","shell.execute_reply.started":"2025-08-21T19:02:30.263699Z","shell.execute_reply":"2025-08-21T19:02:30.270366Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"## Evaluation protocol (fast, decode-free)\n\nDuring training we run a **decode-free evaluator** that scores the **log-probability of the first generated token** for `A/B/C/D` (with both `A` and `\" A\"` tokenizations). This closely matches the answer-only training format and is **much faster** than full generation. We also sample a few explanation outputs after training for qualitative checks.\n","metadata":{}},{"cell_type":"code","source":"@torch.no_grad()\ndef eval_mcq_logits(model, tokenizer, ds, max_items=600, batch_size=2, max_len=768):\n    \"\"\"\n    Fast, regex-free MCQ evaluator that matches your training chat format.\n    - Handles HF Dataset slicing (dict-of-lists) and list-of-rows.\n    - Scores next-token log-prob for A/B/C/D (both 'A' and ' A' tokenizations).\n    - Expects ANSWER_ONLY_PROMPT to end with 'Answer: ' (note trailing space).\n    \"\"\"\n    import torch\n\n    # ---- 0) Limit set size\n    N = min(len(ds), max_items)\n    if N == 0:\n        return 0.0\n\n    # ---- 1) Cache candidate token ids per tokenizer\n    cache_key = getattr(tokenizer, \"name_or_path\", None) or id(tokenizer)\n    _cache = getattr(eval_mcq_logits, \"_cache\", {})\n    if cache_key not in _cache:\n        letters = [\"A\", \"B\", \"C\", \"D\"]\n        cand_ids_no = []\n        cand_ids_sp = []\n        for L in letters:\n            ids0 = tokenizer(L, add_special_tokens=False).input_ids\n            ids1 = tokenizer(\" \" + L, add_special_tokens=False).input_ids\n            cand_ids_no.append(ids0[0] if ids0 else -1)\n            cand_ids_sp.append(ids1[0] if ids1 else -1)\n        _cache[cache_key] = {\"no\": cand_ids_no, \"sp\": cand_ids_sp}\n        eval_mcq_logits._cache = _cache\n\n    cand_ids_no = torch.tensor(_cache[cache_key][\"no\"], device=model.device)\n    cand_ids_sp = torch.tensor(_cache[cache_key][\"sp\"], device=model.device)\n\n    # ---- 2) Helper: build a single chat-formatted prompt\n    def build_prompt(q, a, b, c, d):\n        user = ANSWER_ONLY_PROMPT.format(question=q, opa=a, opb=b, opc=c, opd=d)\n        messages = [\n            {\"role\": \"system\", \"content\": \"You are a medical expert.\"},\n            {\"role\": \"user\",   \"content\": user},\n        ]\n        return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n    correct = 0\n    total = 0\n    model.eval()\n\n    # ---- 3) Iterate in minibatches\n    for i in range(0, N, batch_size):\n        j = min(i + batch_size, N)\n\n        # Slice; HF Dataset returns dict-of-lists here\n        subset = ds[i:j]\n\n        if isinstance(subset, dict):\n            rows = list(zip(\n                subset[\"question\"], subset[\"opa\"], subset[\"opb\"],\n                subset[\"opc\"], subset[\"opd\"], subset[\"cop\"],\n            ))\n        else:\n            # Fallback: list of row dicts (or another sequence-like)\n            rows = [\n                (subset[k][\"question\"], subset[k][\"opa\"], subset[k][\"opb\"],\n                 subset[k][\"opc\"], subset[k][\"opd\"], subset[k][\"cop\"])\n                for k in range(len(subset))\n            ]\n\n        prompts = [build_prompt(q, a, b, c, d) for (q, a, b, c, d, _) in rows]\n\n        inputs = tokenizer(\n            prompts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=max_len,\n        ).to(model.device)\n\n        out = model(**inputs)\n        logits = out.logits  # [B, T, V]\n\n        # Position of first generated token (right after prompt)\n        last_idx = (inputs[\"attention_mask\"].sum(dim=1) - 1)  # [B]\n        first_logits = logits[torch.arange(logits.size(0), device=logits.device), last_idx]  # [B, V]\n        logp = torch.log_softmax(first_logits, dim=-1)  # [B, V]\n\n        # Gather nospace/space variant scores in one go; take max across variants\n        minus_inf = torch.finfo(logp.dtype).min\n        def gather_or_inf(idx_vec):\n            ok = idx_vec >= 0\n            out = torch.full((logp.size(0), 4), minus_inf, device=logp.device, dtype=logp.dtype)\n            if ok.any():\n                out[:, ok] = logp.index_select(1, idx_vec[ok])\n            return out\n\n        scores = torch.maximum(gather_or_inf(cand_ids_no), gather_or_inf(cand_ids_sp))  # [B,4]\n        preds = scores.argmax(dim=1).tolist()\n\n        golds = [\"ABCD\".index(gold_to_letter(cop)) for (_, _, _, _, _, cop) in rows]\n        correct += sum(int(p == g) for p, g in zip(preds, golds))\n        total   += len(rows)\n\n    return correct / max(total, 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T19:02:30.503080Z","iopub.execute_input":"2025-08-21T19:02:30.503301Z","iopub.status.idle":"2025-08-21T19:02:30.516286Z","shell.execute_reply.started":"2025-08-21T19:02:30.503285Z","shell.execute_reply":"2025-08-21T19:02:30.515574Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# ---------- Filter: keep only rows with a valid, parseable gold label ----------\ndef _keep_valid(batch):\n    return [gold_to_letter(c) is not None for c in batch[\"cop\"]]\n\ndef filter_if_labeled(ds, name=\"\"):\n    if name in (\"train\", \"val\"):\n        old = len(ds)\n        ds = ds.filter(_keep_valid, batched=True, num_proc=2, desc=f\"Filter {name}\")\n        print(f\"[label filter] {name}: {old} -> {len(ds)}\")\n    else:\n        # test labels are hidden; keep for inference\n        print(f\"[label filter] {name}: skipped (kept for predictions; labels hidden)\")\n    return ds\n\n# Report sizes before filtering\nn_train0, n_val0_orig = len(train_ds), len(val_ds_orig)\n\ntrain_ds = filter_if_labeled(train_ds, \"train\")\nval_ds_orig   = filter_if_labeled(val_ds_orig,   \"val\")\n\n# Report sizes after filtering\nn_train1, n_val1_orig = len(train_ds), len(val_ds_orig)\nprint(f\"[label filter] train: {n_train0} -> {n_train1} | val: {n_val0_orig} -> {n_val1_orig}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T19:02:32.675774Z","iopub.execute_input":"2025-08-21T19:02:32.676362Z","iopub.status.idle":"2025-08-21T19:02:33.488149Z","shell.execute_reply.started":"2025-08-21T19:02:32.676336Z","shell.execute_reply":"2025-08-21T19:02:33.487285Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter train (num_proc=2):   0%|          | 0/182822 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33f133031f6d47e5a079222eee79eb81"}},"metadata":{}},{"name":"stdout","text":"[label filter] train: 182822 -> 182822\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter val (num_proc=2):   0%|          | 0/4183 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28fd5bfe51fb473f967c31bb7ae4a2ea"}},"metadata":{}},{"name":"stdout","text":"[label filter] val: 4183 -> 4183\n[label filter] train: 182822 -> 182822 | val: 4183 -> 4183\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"### Step 3 ‚Äî Shortening the Dataset\n\n#### Train/val subject selection\n\nTo keep runs feasible on T4, we **automatically pick the ‚Äúmiddle-2‚Äù subjects by frequency** in the training split, and then stratify a 70/30 train/val split on `subject_name`. This keeps train/val distributions similar.\n\n> If you want to pin specific subjects (e.g., ‚ÄúPhysiology‚Äù, ‚ÄúBiochemistry‚Äù), replace the middle-2 selection with a fixed list.\n\n**Purpose:** Done to reduce the size of the dataset, so that it can be trained using Kaggle free compute resources","metadata":{}},{"cell_type":"markdown","source":"**Note:** The validation dataset is obtained from splitting the train set. Original validation split is treated as test here (original test has no labels).\n\n**Reason**\n\nThe original test dataset doesn't contain label to measure the accuracy. Hence, the existing validation dataset is used as an alternative for comparison","metadata":{}},{"cell_type":"code","source":"from collections import Counter\n\n# 0) sanity: make sure column exists\nassert \"subject_name\" in train_ds.column_names, \"subject_name column missing\"\n\n# 1) pick middle-2\nsubj_counts = Counter(train_ds[\"subject_name\"])\nordered = [s for s, _ in sorted(subj_counts.items(), key=lambda kv: kv[1], reverse=True)]\nif len(ordered) < 2:\n    raise ValueError(f\"Need ‚â•2 subjects, found {len(ordered)}\")\n\nmid = len(ordered) // 2\nmiddle2_subjects = ordered[max(0, mid - 1): mid + 1]  # 2 around the median\nprint(\"Middle-2 subjects (by train frequency):\", middle2_subjects)\n\n# 2) filter splits (IID: keep only these subjects)\nkeep = lambda ex: ex[\"subject_name\"] in middle2_subjects\ntrain_ds     = train_ds.filter(keep, num_proc=2)\nval_ds_orig  = val_ds_orig.filter(keep, num_proc=2)\n\nprint(f\"[middle2 filter] sizes -> train: {len(train_ds)} | val_orig: {len(val_ds_orig)}\")\nprint(\"subjects (train):\", sorted(set(train_ds[\"subject_name\"])))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T19:02:33.489967Z","iopub.execute_input":"2025-08-21T19:02:33.490295Z","iopub.status.idle":"2025-08-21T19:02:38.614770Z","shell.execute_reply.started":"2025-08-21T19:02:33.490270Z","shell.execute_reply":"2025-08-21T19:02:38.614053Z"}},"outputs":[{"name":"stdout","text":"Middle-2 subjects (by train frequency): ['Physiology', 'Biochemistry']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter (num_proc=2):   0%|          | 0/182822 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba2976feb0e34c04a5784e5a89456ba4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter (num_proc=2):   0%|          | 0/4183 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc00d8fab47e45a9b52fb326c09a27b3"}},"metadata":{}},{"name":"stdout","text":"[middle2 filter] sizes -> train: 17112 | val_orig: 342\nsubjects (train): ['Biochemistry', 'Physiology']\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# Encode subject_name to ClassLabel so we can stratify\ntrain_ds = train_ds.class_encode_column(\"subject_name\")\n\nsplit = train_ds.train_test_split(\n    test_size=0.3,                  # 30% of train ‚Üí val\n    seed=3407,\n    stratify_by_column=\"subject_name\"\n)\n\ntrain_ds, val_ds = split[\"train\"], split[\"test\"]\n\nprint(f\"[new split] train: {len(train_ds)} | val: {len(val_ds)}\")\nprint(\"Subjects (train):\", sorted(set(train_ds[\"subject_name\"])))\nprint(\"Subjects (val):\", sorted(set(val_ds[\"subject_name\"])))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T19:02:38.615792Z","iopub.execute_input":"2025-08-21T19:02:38.616129Z","iopub.status.idle":"2025-08-21T19:02:41.586925Z","shell.execute_reply.started":"2025-08-21T19:02:38.616100Z","shell.execute_reply":"2025-08-21T19:02:41.586175Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Flattening the indices:   0%|          | 0/17112 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cb9b99db2774212bf8092e8dd8ff090"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Casting to class labels:   0%|          | 0/17112 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45a70e1a58ab4e8fb8a8fc4c6aef2d04"}},"metadata":{}},{"name":"stdout","text":"[new split] train: 11978 | val: 5134\nSubjects (train): [0, 1]\nSubjects (val): [0, 1]\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"print(\"[label sanity] train:\", Counter(gold_to_letter(x) for x in train_ds[\"cop\"]))\nprint(\"[label sanity] val_orig:\", Counter(gold_to_letter(x) for x in val_ds_orig[\"cop\"]))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T19:02:41.588404Z","iopub.execute_input":"2025-08-21T19:02:41.588615Z","iopub.status.idle":"2025-08-21T19:02:41.642515Z","shell.execute_reply.started":"2025-08-21T19:02:41.588592Z","shell.execute_reply":"2025-08-21T19:02:41.641981Z"}},"outputs":[{"name":"stdout","text":"[label sanity] train: Counter({'A': 3379, 'B': 3193, 'C': 2826, 'D': 2580})\n[label sanity] val_orig: Counter({'A': 103, 'C': 86, 'B': 77, 'D': 76})\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"### Checkpoints & logging\n\n- Checkpoints are saved under `/kaggle/working/outputs/<model-name>/`.  \n  If you are not on Kaggle, change `save_dir` to a local folder like `outputs/<model-name>/`.\n- Weights & Biases logging is enabled via `WANDB_API_KEY` (secret name: **`wnb`**).  \n  If not set, disable/report-to by setting `report_to=None` in `TrainingArguments`.\n","metadata":{}},{"cell_type":"markdown","source":"### Step 4 ‚Äî Fine-tuning the model\n\nThis block fine-tunes the model and picks the best iteration based on validation accuracy.","metadata":{}},{"cell_type":"code","source":"print(f\"\\n=== Fine-tuning {base_name} ===\")\n\n# free leftovers from any previous attempts / previous base\nfor var in (\"trainer\",\"model_lora\",\"model\",\"tokenizer\"):\n    if var in globals():\n        try:\n            del globals()[var]\n        except:\n            pass\ngc.collect()\ntorch.cuda.empty_cache()\n\nmodel, tokenizer = load_base(base_name)\n\n# padding config FIRST\ntokenizer.padding_side = \"left\"\ntokenizer.truncation_side = \"left\"\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\ntokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\nmodel.config.pad_token_id = tokenizer.pad_token_id\nmodel.generation_config.pad_token_id = tokenizer.pad_token_id\nmodel.generation_config.eos_token_id = tokenizer.eos_token_id\n\nprint(\"Baseline (no LoRA) small-val:\",\n      eval_mcq_logits(model, tokenizer, val_ds_orig, max_items=200, batch_size=2, max_len=768))\n\nrun = wandb.init(\n    project=\"medmcqa-finetune-reduced-updated\",\n    job_type=\"training\",\n    name = base_name.split(\"/\")[-1].replace(\"-\", \"_\"),\n    reinit=True\n)\n\neos_token = tokenizer.eos_token or \"</s>\"\ntrain_finetune = train_ds.map(\n    lambda b: to_text_examples(b, tokenizer, eos_token=tokenizer.eos_token),\n    batched=True\n)\n\nmodel_lora = add_lora(model)\nsafe_name = base_name.split(\"/\")[-1].lower().replace(\" \", \"-\")\n\nprint(\"TRAIN EXAMPLE:\\n\", train_finetune[0][\"text\"][:400])\n\n# sanity: trainable params > 0\ntrainable = sum(p.numel() for p in model_lora.parameters() if p.requires_grad)\ntotal     = sum(p.numel() for p in model_lora.parameters())\nprint(f\"Trainable params: {trainable:,} / {total:,}\")\n\n# pass eval_dataset=None; we validate via callback + eval_mcq\nmodel.config.use_cache = False\nsave_dir = f\"/kaggle/working/outputs/{safe_name}\"\ntrainer = make_trainer(model_lora, tokenizer, train_finetune, outdir=save_dir, eval_dataset=None)\ntrainer.add_callback(MCQAccuracyCallback(\n    tokenizer, val_ds, every=200, patience=8,\n    max_items=600, save_dir=save_dir\n))\n_ = trainer.train()\n\n# === reload best checkpoint if it exists ===\nbest_dir = os.path.join(save_dir, \"best\")\nif os.path.isdir(best_dir):\n    from peft import PeftModel\n    model_best = PeftModel.from_pretrained(model, best_dir)\n    print(f\"Loaded best adapter from {best_dir}\")\nelse:\n    print(\"‚ö†Ô∏è No 'best' dir found, using last-step weights.\")\n    model_best = model_lora\n\n# Evaluate on validation (answer-only, batched)\nval_acc = eval_mcq_logits(model_best, tokenizer, val_ds, max_items=300, batch_size=2, max_len=768)\n\n# ‚úÖ log with the current step so it lands on the same x-axis\nwandb.log({\"val_accuracy\": val_acc}, step=int(trainer.state.global_step))\nprint(f\"Validation accuracy (A/B/C/D) for {base_name}: {val_acc:.3f}\")\n\n# small explanation probe\nsample_explanations(model_best, tokenizer, val_ds, k=8, batch_size=2, new_tokens=160)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T19:02:41.643357Z","iopub.execute_input":"2025-08-21T19:02:41.643559Z","iopub.status.idle":"2025-08-21T23:00:22.123639Z","shell.execute_reply.started":"2025-08-21T19:02:41.643543Z","shell.execute_reply":"2025-08-21T23:00:22.122871Z"}},"outputs":[{"name":"stdout","text":"\n=== Fine-tuning meta-llama/Meta-Llama-3-8B-Instruct ===\n==((====))==  Unsloth 2025.8.9: Fast Llama patching. Transformers: 4.55.3.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.8.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.4.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.32.post2. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/5.70G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b77043fd6f94ee289bd68b6206b8e4e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/220 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccd6dd06abbf4615be43755e88556cf3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9b2e8a4583d419988fc2c8f722d5536"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e73538578b1e4265b3f0c9e499f833ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/345 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79deeffdaaf84ba986502125fb57a235"}},"metadata":{}},{"name":"stdout","text":"Baseline (no LoRA) small-val: 0.43\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.20.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250821_190402-o6vgqfv2</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/avpk-university-of-waterloo/medmcqa-finetune-reduced-updated/runs/o6vgqfv2' target=\"_blank\">Meta_Llama_3_8B_Instruct</a></strong> to <a href='https://wandb.ai/avpk-university-of-waterloo/medmcqa-finetune-reduced-updated' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/avpk-university-of-waterloo/medmcqa-finetune-reduced-updated' target=\"_blank\">https://wandb.ai/avpk-university-of-waterloo/medmcqa-finetune-reduced-updated</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/avpk-university-of-waterloo/medmcqa-finetune-reduced-updated/runs/o6vgqfv2' target=\"_blank\">https://wandb.ai/avpk-university-of-waterloo/medmcqa-finetune-reduced-updated/runs/o6vgqfv2</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11978 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1043a94527c4275830947d1487e0cc5"}},"metadata":{}},{"name":"stderr","text":"Unsloth 2025.8.9 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n","output_type":"stream"},{"name":"stdout","text":"TRAIN EXAMPLE:\n <|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nYou are a medical expert.<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nYou are a medical expert. Answer this MCQ with a single letter.\n\nQuestion:\nWhich one of these is absorbed in ileum?\n\nOptions:\nA. Vitamin D\nB. B12\nC. Iron\nD. Fat\n\nRespond in the format:\nAnswer: <A/B/C/D><|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nAns\nTrainable params: 83,886,080 / 4,624,486,400\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Unsloth: Tokenizing [\"text\"]:   0%|          | 0/11978 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e272e3fccb004145b444cc498315a201"}},"metadata":{}},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 11,978 | Num Epochs = 2 | Total steps = 1,498\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 8\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 8 x 1) = 16\n \"-____-\"     Trainable parameters = 83,886,080 of 8,114,147,328 (1.03% trained)\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1498' max='1498' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1498/1498 3:53:00, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>50</td>\n      <td>1.321400</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>0.608300</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>0.586600</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>0.589200</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.580600</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.564400</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.572300</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.574000</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.561100</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.556200</td>\n    </tr>\n    <tr>\n      <td>550</td>\n      <td>0.565400</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>0.552500</td>\n    </tr>\n    <tr>\n      <td>650</td>\n      <td>0.567300</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>0.542100</td>\n    </tr>\n    <tr>\n      <td>750</td>\n      <td>0.534300</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>0.394400</td>\n    </tr>\n    <tr>\n      <td>850</td>\n      <td>0.404700</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>0.404100</td>\n    </tr>\n    <tr>\n      <td>950</td>\n      <td>0.404400</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.411700</td>\n    </tr>\n    <tr>\n      <td>1050</td>\n      <td>0.398300</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>0.390800</td>\n    </tr>\n    <tr>\n      <td>1150</td>\n      <td>0.383700</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>0.374600</td>\n    </tr>\n    <tr>\n      <td>1250</td>\n      <td>0.377100</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>0.383200</td>\n    </tr>\n    <tr>\n      <td>1350</td>\n      <td>0.380900</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>0.382100</td>\n    </tr>\n    <tr>\n      <td>1450</td>\n      <td>0.380600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"[step 200] accuracy 0.7183\nNew best acc=0.7183, saving to /kaggle/working/outputs/meta-llama-3-8b-instruct/best\n[step 400] accuracy 0.8383\nNew best acc=0.8383, saving to /kaggle/working/outputs/meta-llama-3-8b-instruct/best\n[step 600] accuracy 0.7867\n[step 800] accuracy 0.8150\n[step 1000] accuracy 0.8233\n[step 1200] accuracy 0.8150\n[step 1400] accuracy 0.8217\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py:167: UserWarning: Already found a `peft_config` attribute in the model. This will lead to having multiple adapters in the model. Make sure to know what you are doing!\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Loaded best adapter from /kaggle/working/outputs/meta-llama-3-8b-instruct/best\nValidation accuracy (A/B/C/D) for meta-llama/Meta-Llama-3-8B-Instruct: 0.857\n","output_type":"stream"},{"name":"stderr","text":"A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\nA decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n","output_type":"stream"},{"name":"stdout","text":"[0] gold=C pred=C correct=True\n  explanation: Carbon monoxide poisoning causes respiratory depression and decreased ventilation, not hyperventilation. Hyperventilation is a response to respiratory alkalosis, which is caused by increased ventilati...\n\n[1] gold=B pred=B correct=True\n  explanation: The prime driving force for counter current multiplier system is the reabsorption of Na+ in thick ascending limb. This is because Na+ is reabsorbed in a countercurrent manner, which creates a high osm...\n\n[2] gold=D pred=D correct=True\n  explanation: Lesch-Nyhan syndrome is a rare genetic disorder caused by deficiency of HGPRTase enzyme. It is characterized by hyperuricemia, gouty ahritis, and self-mutilation. Febuxostat is a xanthine oxidase inhi...\n\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"### Step 5 ‚Äî Testing the model on the test-dataset\n\nThis block involves testing the best model on the test dataset and storing the parameters\n\n**Note:** As mentioned earlier, in this workflow, original validation dataset is considered as the test dataset","metadata":{}},{"cell_type":"code","source":"val_acc_orig = eval_mcq_logits(model_best, tokenizer, val_ds_orig, max_items=len(val_ds_orig), batch_size=2, max_len=768)\nprint(f\"[FINAL] Original validation accuracy (best): {val_acc_orig:.4f}\")\nwandb.log({\"final_val_orig_accuracy\": val_acc_orig}, step=int(trainer.state.global_step))\n\n# Save final best adapter to a clean dir\nfinal_dir = f\"{safe_name}-medmcqa-lora-best\"\nmodel_best.save_pretrained(final_dir)\ntokenizer.save_pretrained(final_dir)\nprint(f\"Saved BEST LoRA to: {final_dir}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T23:00:22.125154Z","iopub.execute_input":"2025-08-21T23:00:22.125374Z","iopub.status.idle":"2025-08-21T23:01:27.352810Z","shell.execute_reply.started":"2025-08-21T23:00:22.125355Z","shell.execute_reply":"2025-08-21T23:01:27.352161Z"}},"outputs":[{"name":"stdout","text":"[FINAL] Original validation accuracy (best): 0.6520\nSaved BEST LoRA to: meta-llama-3-8b-instruct-medmcqa-lora-best\n","output_type":"stream"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Tried to log to step 1498 that is less than the current step 1499. Steps must be monotonically increasing, so this data will be ignored. See https://wandb.me/define-metric to log data out of order.\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"run.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-21T23:03:53.972392Z","iopub.execute_input":"2025-08-21T23:03:53.972981Z","iopub.status.idle":"2025-08-21T23:03:54.394497Z","shell.execute_reply.started":"2025-08-21T23:03:53.972953Z","shell.execute_reply":"2025-08-21T23:03:54.393815Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà</td></tr><tr><td>train/global_step</td><td>‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà</td></tr><tr><td>train/grad_norm</td><td>‚ñà‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÜ‚ñÑ‚ñÉ‚ñÖ‚ñÉ‚ñÑ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ</td></tr><tr><td>train/learning_rate</td><td>‚ñÜ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>train/loss</td><td>‚ñà‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ</td></tr><tr><td>val_accuracy</td><td>‚ñÅ‚ñá‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñà</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1.121026409543762e+17</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/global_step</td><td>1498</td></tr><tr><td>train/grad_norm</td><td>1.02963</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3806</td></tr><tr><td>train_loss</td><td>0.5042</td></tr><tr><td>train_runtime</td><td>13995.0318</td></tr><tr><td>train_samples_per_second</td><td>1.712</td></tr><tr><td>train_steps_per_second</td><td>0.107</td></tr><tr><td>val_accuracy</td><td>0.85667</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">Meta_Llama_3_8B_Instruct</strong> at: <a href='https://wandb.ai/avpk-university-of-waterloo/medmcqa-finetune-reduced-updated/runs/o6vgqfv2' target=\"_blank\">https://wandb.ai/avpk-university-of-waterloo/medmcqa-finetune-reduced-updated/runs/o6vgqfv2</a><br> View project at: <a href='https://wandb.ai/avpk-university-of-waterloo/medmcqa-finetune-reduced-updated' target=\"_blank\">https://wandb.ai/avpk-university-of-waterloo/medmcqa-finetune-reduced-updated</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250821_190402-o6vgqfv2/logs</code>"},"metadata":{}}],"execution_count":25}]}